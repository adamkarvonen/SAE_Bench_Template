{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from graphing_utils import plot_2var_graph, plot_3var_graph, plot_interactive_3var_graph, plot_steps_vs_average_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity_penalty(config: dict, trainer_class: str) -> float:\n",
    "    if trainer_class == \"TrainerTopK\":\n",
    "        return config[\"trainer\"][\"k\"]\n",
    "    elif trainer_class == \"PAnnealTrainer\":\n",
    "        return config[\"trainer\"][\"sparsity_penalty\"]\n",
    "    else:\n",
    "        return config[\"trainer\"][\"l1_penalty\"]\n",
    "\n",
    "\n",
    "def ae_config_results(ae_paths: list[str], dictionaries_path: str) -> dict[str, dict[str, float]]:\n",
    "    results = {}\n",
    "    for ae_path in ae_paths:\n",
    "        config_file = f\"{ae_path}/config.json\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        ae_name = ae_path.split(dictionaries_path)[1]\n",
    "\n",
    "        results[ae_name] = {}\n",
    "\n",
    "        trainer_class = config[\"trainer\"][\"trainer_class\"]\n",
    "        results[ae_name][\"trainer_class\"] = trainer_class\n",
    "        results[ae_name][\"l1_penalty\"] = get_sparsity_penalty(config, trainer_class)\n",
    "\n",
    "        results[ae_name][\"lr\"] = config[\"trainer\"][\"lr\"]\n",
    "        results[ae_name][\"dict_size\"] = config[\"trainer\"][\"dict_size\"]\n",
    "        if \"steps\" in config[\"trainer\"]:\n",
    "            results[ae_name][\"steps\"] = config[\"trainer\"][\"steps\"]\n",
    "        else:\n",
    "            results[ae_name][\"steps\"] = -1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def add_custom_metric_results(\n",
    "    ae_paths: list[str],\n",
    "    results: dict[str, dict[str, float]],\n",
    "    metric_filename: str,\n",
    "    dictionaries_path: str,\n",
    "    metric_dict_key: Optional[str] = None,\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    for ae_path in ae_paths:\n",
    "        config_file = f\"{ae_path}/{metric_filename}\"\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            custom_metric_results = json.load(f)\n",
    "\n",
    "        ae_name = ae_path.split(dictionaries_path)[1]\n",
    "\n",
    "        if metric_dict_key:\n",
    "            results[ae_name][\"custom_metric\"] = custom_metric_results[metric_dict_key]\n",
    "        else:\n",
    "            for key, value in custom_metric_results.items():\n",
    "                results[ae_name][key] = value\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_l0_threshold(results: dict, l0_threshold: Optional[int]) -> dict:\n",
    "    if l0_threshold is not None:\n",
    "        filtered_results = {\n",
    "            path: data for path, data in results.items() if data[\"l0\"] <= l0_threshold\n",
    "        }\n",
    "\n",
    "        # Optional: Print how many results were filtered out\n",
    "        filtered_count = len(results) - len(filtered_results)\n",
    "        print(f\"Filtered out {filtered_count} results with L0 > {l0_threshold}\")\n",
    "\n",
    "        # Replace the original results with the filtered results\n",
    "        results = filtered_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename_prefix = \"images/\"\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_name = \"sae_bench_pythia70m_sweep_topk_ctx128_0730\"\n",
    "\n",
    "folder_path = \"sparse_probing/src/sparse_probing_results\"\n",
    "filename = f\"example_results_{release_name}_eval_results.json\"\n",
    "\n",
    "filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "with open(filepath, \"r\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results.keys())\n",
    "print(eval_results[\"custom_eval_results\"].keys())\n",
    "print(eval_results[\"custom_eval_results\"][\"pythia70m_sweep_topk_ctx128_0730/resid_post_layer_4/trainer_0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_data_filename = f\"sae_bench_data/{release_name}_data.json\"\n",
    "\n",
    "with open(sae_data_filename, \"r\") as f:\n",
    "    sae_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sae_data.keys())\n",
    "print(sae_data[\"basic_eval_results\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_results = {}\n",
    "k= 100\n",
    "\n",
    "custom_metric = f'sae_top_{k}_test_accuracy'\n",
    "\n",
    "for sae_name in eval_results['custom_eval_results']:\n",
    "    plotting_results[sae_name] = {}\n",
    "\n",
    "    plotting_results[sae_name]['trainer_class'] = sae_data['sae_config_dictionary_learning'][sae_name][\"trainer\"][\"trainer_class\"]\n",
    "    plotting_results[sae_name]['l0'] = sae_data['basic_eval_results'][sae_name]['l0']\n",
    "    plotting_results[sae_name]['frac_recovered'] = sae_data['basic_eval_results'][sae_name]['frac_recovered']\n",
    "\n",
    "    plotting_results[sae_name][custom_metric] = eval_results['custom_eval_results'][sae_name][custom_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_metric_name = f\"{k}-Sparse Probe Accuracy\"\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric_name}\"\n",
    "\n",
    "plot_3var_graph(\n",
    "    plotting_results,\n",
    "    title,\n",
    "    custom_metric,\n",
    "    colorbar_label=\"Custom Metric\",\n",
    "    output_filename=f\"{image_filename_prefix}{custom_metric}_3var.png\",\n",
    ")\n",
    "plot_2var_graph(\n",
    "    plotting_results,\n",
    "    custom_metric,\n",
    "    title=title,\n",
    "    output_filename=f\"{image_filename_prefix}{custom_metric}_2var.png\",\n",
    ")\n",
    "# plot_interactive_3var_graph(plotting_results, custom_metric)\n",
    "\n",
    "# At this point, if there's any additional .json files located alongside the ae.pt and eval_results.json\n",
    "# You can easily adapt them to be included in the plotting_results dictionary by using something similar to add_ae_config_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_names: list[str],\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Correlation Heatmap\",\n",
    "    output_filename: str = None,\n",
    "    figsize: tuple = (12, 10),\n",
    "    cmap: str = \"coolwarm\",\n",
    "    annot: bool = True,\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # If metric_names is not provided, use all metric names from the first ae_name\n",
    "    # if metric_names is None:\n",
    "    #     metric_names = list(plotting_results[ae_names[0]].keys())\n",
    "\n",
    "    # Create a DataFrame from the plotting_results\n",
    "    data = []\n",
    "    for ae in ae_names:\n",
    "        row = [plotting_results[ae].get(metric, np.nan) for metric in metric_names]\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, index=ae_names, columns=metric_names)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, annot=annot, cmap=cmap, vmin=-1, vmax=1, center=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_10_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_10_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def plot_metric_scatter(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_x: str,\n",
    "    metric_y: str,\n",
    "    x_label: Optional[str] = None,\n",
    "    y_label: Optional[str] = None,\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Comparison Scatter Plot\",\n",
    "    output_filename: Optional[str] = None,\n",
    "    figsize: tuple = (10, 8),\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # Extract x and y values for the specified metrics\n",
    "    x_values = [plotting_results[ae].get(metric_x, float(\"nan\")) for ae in ae_names]\n",
    "    y_values = [plotting_results[ae].get(metric_y, float(\"nan\")) for ae in ae_names]\n",
    "\n",
    "    # Remove any NaN values\n",
    "    valid_data = [\n",
    "        (x, y, ae)\n",
    "        for x, y, ae in zip(x_values, y_values, ae_names)\n",
    "        if not (np.isnan(x) or np.isnan(y))\n",
    "    ]\n",
    "    if not valid_data:\n",
    "        print(\"No valid data points after removing NaN values.\")\n",
    "        return\n",
    "\n",
    "    x_values, y_values, valid_ae_names = zip(*valid_data)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "\n",
    "    # Calculate correlation coefficients\n",
    "    r, p_value = stats.pearsonr(x_values, y_values)\n",
    "    r_squared = r**2\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    scatter = sns.scatterplot(x=x_values, y=y_values, label=\"SAE\", color=\"blue\")\n",
    "\n",
    "    if x_label is None:\n",
    "        x_label = metric_x\n",
    "    if y_label is None:\n",
    "        y_label = metric_y\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add a trend line\n",
    "    sns.regplot(x=x_values, y=y_values, scatter=False, color=\"red\", label=f\"r = {r:.4f}\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print correlation coefficients\n",
    "    print(f\"Pearson correlation coefficient (r): {r:.4f}\")\n",
    "    print(f\"Coefficient of determination (rÂ²): {r_squared:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# plot_metric_scatter(plotting_results, metric_x=\"l0\", metric_y=\"frac_recovered\", title=\"L0 vs Fraction Recovered\")\n",
    "\n",
    "metric1 = f\"tpp_auto_interp_threshold_{threshold}_total_metric\"\n",
    "metric2 = f\"tpp_attrib_threshold_{threshold}_total_metric\"\n",
    "title = f\"\"\n",
    "\n",
    "x_label = \"TPP with LLM Judge\"\n",
    "y_label = \"TPP without LLM Judge\"\n",
    "output_filename = f\"{image_filename_prefix}tpp_comparison_{threshold}_{model_name}.png\"\n",
    "\n",
    "plot_metric_scatter(\n",
    "    plotting_results,\n",
    "    metric_x=metric1,\n",
    "    metric_y=metric2,\n",
    "    title=title,\n",
    "    x_label=x_label,\n",
    "    y_label=y_label,\n",
    "    output_filename=output_filename,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(plotting_results.keys()))\n",
    "print(plotting_results[first_key].keys())\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_20_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_20_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_20\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    \"scr_bias_shift_dir1_threshold_20\",\n",
    "    \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_20\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    \"scr_attrib_dir1_threshold_20\",\n",
    "    \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    # \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    # \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric = f\"scr_bias_shift_dir1_threshold_{threshold}\"\n",
    "custom_metric = f\"scr_attrib_dir2_threshold_{threshold}\"\n",
    "\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric}\"\n",
    "\n",
    "plot_3var_graph(plotting_results, title, custom_metric)\n",
    "plot_2var_graph(plotting_results, custom_metric, title=title, y_label=\"Custom Metric\")\n",
    "plot_interactive_3var_graph(plotting_results, custom_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plotting_results[first_key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = f\"scr_bias_shift_dir1_threshold_{threshold}\"\n",
    "metric2 = f\"scr_attrib_dir1_threshold_{threshold}\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "title = \"\"\n",
    "\n",
    "output_filename = f\"{image_filename_prefix}scr_comparison_{threshold}_{model_name}.png\"\n",
    "\n",
    "plot_metric_scatter(\n",
    "    plotting_results,\n",
    "    metric_x=metric1,\n",
    "    metric_y=metric2,\n",
    "    title=title,\n",
    "    x_label=\"SHIFT with LLM Judge\",\n",
    "    y_label=\"SHIFT without LLM Judge\",\n",
    "    output_filename=output_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = f\"tpp_auto_interp_threshold_{threshold}_total_metric\"\n",
    "metric2 = f\"scr_attrib_dir1_threshold_{threshold}\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "plot_metric_scatter(plotting_results, metric_x=metric1, metric_y=metric2, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
