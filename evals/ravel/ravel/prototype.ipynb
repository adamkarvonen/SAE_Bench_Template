{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from common_imports import *\n",
    "from tqdm import tqdm\n",
    "from ravel_dataset_builder import Prompt\n",
    "from utils.intervention_utils import find_positions_in_tokens\n",
    "from ravel_dataset_builder import RAVELEntityPromptData, evaluate_completion\n",
    "from utils.differential_binary_masking import DifferentialBinaryMasking\n",
    "from tlens_utils import Node\n",
    "from sparse_control_utils import *\n",
    "from mandala.imports import sess\n",
    "from tlens_utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_DEVICE = torch.device(\"cuda:0\")\n",
    "INFERENCE_DEVICE = torch.device(\"cuda:1\") # Gemma-2B is too big \n",
    "def get_model(model_id: str,) -> HookedTransformer:\n",
    "    llm_dtype = torch.float32 # weird non-determinism happens in bfloat16\n",
    "    model = HookedTransformer.from_pretrained_no_processing(model_id, device=MODEL_DEVICE, dtype=llm_dtype, \n",
    "                                                            attn_implementation='eager' # to avoid non-determinism potentially?\n",
    "                                                            )\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# MODEL_ID = 'gemma-2-2b'\n",
    "MODEL_ID = 'pythia-70m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and filter RAVEL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference\n",
    "RAVEL_ENTITIES = ('city', 'nobel_prize_winner', 'occupation', 'physical_object', 'verb')\n",
    "full_entity_dataset = RAVELEntityPromptData.from_files('nobel_prize_winner', 'data', model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_entity_dataset = full_entity_dataset.downsample(8192)\n",
    "print(f\"Number of prompts sampled: {len(sampled_entity_dataset)}\")\n",
    "\n",
    "prompt_max_length = 48\n",
    "batch_size = 64 # use smaller for gemma-2b\n",
    "sampled_entity_dataset.generate_completions(model, model.tokenizer, max_length=prompt_max_length+8, prompt_max_length=prompt_max_length, batch_size=batch_size)\n",
    "sampled_entity_dataset.evaluate_correctness()\n",
    "\n",
    "# Filter correct completions\n",
    "correct_data = sampled_entity_dataset.filter_correct()\n",
    "\n",
    "# Filter top entities and templates\n",
    "filtered_data = correct_data.filter_top_entities_and_templates(top_n_entities=400, top_n_templates_per_attribute=12)\n",
    "\n",
    "# Calculate average accuracy\n",
    "accuracy = sampled_entity_dataset.calculate_average_accuracy()\n",
    "print(f\"Average accuracy: {accuracy:.2%}\")\n",
    "print(f\"Number of prompts remaining: {len(correct_data)}\")\n",
    "print(f\"Number of entitites after filtering: {len(set([p.entity for p in list(correct_data.prompts.values())]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for loading SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../..')\n",
    "import sae_bench_utils.activation_collection as activation_collection\n",
    "import sae_bench_utils.formatting_utils as formatting_utils\n",
    "# import saebench_utils.activation_collection as activation_collection\n",
    "# import saebench_utils.formatting_utils as formatting_utils\n",
    "overview_df = formatting_utils.make_available_sae_df(for_printing=True)\n",
    "from sae_lens import SAE as SAELensSAE\n",
    "from sae_lens.sae import TopK\n",
    "from tlens_utils import Node\n",
    "from sparse_control_utils import *\n",
    "\n",
    "def load_SAELensSAE(\n",
    "        layer: int,\n",
    "        expansion_factor: int = 2,\n",
    "        k: Literal[20, 40, 80, 160, 320, 640] = 40,\n",
    "        variant: Literal['standard', 'topk'] = 'topk',\n",
    "        llm_name: Literal['gemma-2-2b', 'pythia70m'] = 'gemma-2-2b',\n",
    "        ctx: int = 128,\n",
    "        device: str = 'cuda',\n",
    "    ) -> Tuple[SAELensSAE, dict, Optional[Tensor]]:\n",
    "    \"\"\"\n",
    "    Load a pre-trained SAE from SAELens\n",
    "    \"\"\"\n",
    "    k_to_trainer = {20: 0, 40: 1, 80: 2, 160: 3, 320: 4, 640: 5}\n",
    "    trainer = k_to_trainer[k]\n",
    "    # assert llm_name == 'gemma-2-2b', \"only gemma-2-2b is supported for now\"\n",
    "    if llm_name == 'gemma-2-2b':\n",
    "        release = f'sae_bench_{llm_name}_sweep_{variant}_ctx{ctx}_ef{expansion_factor}_0824'\n",
    "        sae_name_prefix = f'{llm_name}_sweep_{variant}_ctx{ctx}_ef{expansion_factor}_0824'\n",
    "    elif llm_name == 'pythia70m':\n",
    "        if variant == 'standard':\n",
    "            suffix = '0712'\n",
    "        else:\n",
    "            suffix = '0730'\n",
    "        release = f'sae_bench_{llm_name}_sweep_{variant}_ctx{ctx}_{suffix}'\n",
    "        sae_name_prefix = f'{llm_name}_sweep_{variant}_ctx{ctx}_{suffix}'\n",
    "    sae_name_suffix = f'resid_post_layer_{layer}/trainer_{trainer}'\n",
    "    sae_df = formatting_utils.make_available_sae_df(for_printing=False)\n",
    "    sae_name = f'{sae_name_prefix}/{sae_name_suffix}'\n",
    "    sae_id_to_name_map = sae_df.saes_map[release]\n",
    "    sae_name_to_id_map = {v: k for k, v in sae_id_to_name_map.items()}\n",
    "    sae_id = sae_name_to_id_map[sae_name]\n",
    "    sae, cfg_dict, sparsity = SAELensSAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=device,\n",
    "    )\n",
    "    sae = sae.to(device=device)\n",
    "    if variant == 'topk':\n",
    "        assert isinstance(sae.activation_fn, TopK), \"This sae is not a topk sae, you probably have an old sae_lens version\"\n",
    "    if llm_name == 'gemma-2-2b':\n",
    "        assert cfg_dict['activation_fn_kwargs']['k'] == k, f\"Expected k={k}, got k={cfg_dict['activation_fn_kwargs']['k']}\"\n",
    "    sae.requires_grad_(False)\n",
    "    return sae, cfg_dict, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_NAME = 'pythia70m'\n",
    "D_MODEL = model.cfg.d_model\n",
    "N_LAYERS = len(model.blocks)\n",
    "SAE_LAYERS = (3, 7, 11, 15, 19, ) if LLM_NAME == 'gemma-2-2b' else (3, 4)\n",
    "LAYER_TO_IDX = {layer: idx for idx, layer in enumerate(SAE_LAYERS)}\n",
    "NODES = {layer: Node(component_name='resid_post', layer=layer, seq_pos=None) for layer in SAE_LAYERS}\n",
    "SAE_NODES = list(NODES.values())\n",
    "ALL_NODES = [Node(component_name='resid_post', layer=layer, seq_pos=None) for layer in range(N_LAYERS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the above\n",
    "sae = load_SAELensSAE(layer=3, k=40, llm_name='pythia70m', ctx=128, device='cuda')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train differential binary masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up prompts and prompt pairs for interchange interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prompt_pairs(prompts: List[Prompt], N: int, random_seed: int = 0, attribute: Optional[str] = None) -> Tuple[List[Prompt], List[Prompt]]:\n",
    "    \"\"\"\n",
    "    Sample pairs of prompts from a list of prompts (should be for the same \n",
    "    entity type). \n",
    "\n",
    "    Optionally, require the prompts in a pair to differ in the value of \n",
    "    a given attribute.\n",
    "    \"\"\"\n",
    "    res_og = []\n",
    "    res_cf = []\n",
    "    np.random.seed(random_seed)\n",
    "    if attribute is None:\n",
    "        for i in range(N):\n",
    "            p1, p2 = np.random.choice(prompts, size=2, replace=False)\n",
    "            res_og.append(p1)\n",
    "            res_cf.append(p2)\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            p1, p2 = np.random.choice(prompts, size=2, replace=False)\n",
    "            attr_dict = sampled_entity_dataset.entity_attributes\n",
    "            while attr_dict[p1.entity][attribute] == attr_dict[p2.entity][attribute]:\n",
    "                p1, p2 = np.random.choice(prompts, size=2, replace=False)\n",
    "            res_og.append(p1)\n",
    "            res_cf.append(p2)\n",
    "    return res_og, res_cf\n",
    "\n",
    "ATTRIBUTE = 'Field'\n",
    "prompts = list(correct_data.prompts.values())\n",
    "prompts = [p for p in prompts if p.attribute == ATTRIBUTE]\n",
    "correct_completions = [correct_data.entity_attributes[p.entity][p.attribute] for p in prompts]\n",
    "last_entity_token_positions = Tensor([get_entity_positions(p, model)[-1] for p in tqdm(prompts)]).long()\n",
    "og_prompts, cf_prompts = sample_prompt_pairs(prompts, N=1000, random_seed=0, attribute=ATTRIBUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbm_hook(dbm: DifferentialBinaryMasking, \n",
    "                 layer: int, positions: Tensor,\n",
    "                 A_og: Tensor, A_cf: Tensor\n",
    "                 ) -> Tuple[str, Callable]:\n",
    "    \"\"\"\n",
    "    Activation patching hook w/ a differential binary mask.\n",
    "\n",
    "    Returns a transformerlens hook that\n",
    "    - takes batches of original and counterfactual activations (from e.g. a\n",
    "    residual stream of the model), and runs them through a differential binary\n",
    "    mask layer to produce an activation to be used in an intervention \n",
    "    - activation-patches the original activations at the given positions with\n",
    "    the intervention activations\n",
    "    \"\"\"\n",
    "    A_intv = dbm.forward(base=A_og, source=A_cf)\n",
    "    node = Node(component_name='resid_post', layer=layer, seq_pos=None)\n",
    "\n",
    "    def hook_fn(activation: Tensor, hook: HookPoint) -> Tensor:\n",
    "        # activation will be of shape (num_texts, seq_len, d_act)\n",
    "        batch_size = activation.shape[0]\n",
    "        activation[:, positions, :] = A_intv.to(activation.device).to(activation.dtype)\n",
    "        return activation\n",
    "    return (node.activation_name, hook_fn)\n",
    "\n",
    "def get_loss_with_hooks(\n",
    "    model: HookedTransformer, \n",
    "    hooks: List[Tuple[str, Callable]],\n",
    "    prompts: List[Prompt],\n",
    "    target_completions: List[str],\n",
    "    n_tokens: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Apply the given hooks, and measure the loss w.r.t. the given completions.\n",
    "\n",
    "    This is used as an optimization objective when training the differential\n",
    "    binary mask. When we want to train a mask that changes the model's output,\n",
    "    the `target_completions` should be the desired completions.\n",
    "    \"\"\"\n",
    "    texts = [p.text for p in prompts]\n",
    "\n",
    "    texts_tokens = model.to_tokens(texts, padding_side='left', prepend_bos=True)\n",
    "    completion_tokens = model.to_tokens(target_completions, padding_side='right', prepend_bos=False)\n",
    "    completion_lengths = [len(model.to_str_tokens(target_completions[i], prepend_bos=False)) for i in range(len(target_completions))]\n",
    "    n_tokens = min(n_tokens, completion_tokens.shape[1])\n",
    "\n",
    "    current_tokens = texts_tokens\n",
    "    losses = []\n",
    "    for i in range(n_tokens):\n",
    "        logits = model.run_with_hooks(current_tokens, fwd_hooks=hooks,)[:, -1, :] # only take the last token logits, shape (batch, vocab)\n",
    "        # find the highest probability token\n",
    "        # next_token_idx = torch.argdax(logits, dim=-1).unsqueeze(1) # shape (batch, 1)\n",
    "        next_token_idx = completion_tokens[:, i].unsqueeze(1) # shape (batch, 1), indices over the vocab\n",
    "        next_token_losses = F.cross_entropy(logits, next_token_idx.squeeze(), reduction='none')\n",
    "        losses.append(next_token_losses)\n",
    "        current_tokens = torch.cat([current_tokens, next_token_idx], dim=-1)\n",
    "    losses = torch.stack(losses, dim=1)\n",
    "    total = sum([losses[i, :completion_lengths[i]].sum() for i in range(len(losses))])\n",
    "    total_count = sum([completion_lengths[i] for i in range(len(completion_lengths))])\n",
    "    avg_loss = total / total_count\n",
    "    # full_texts = [''.join(model.to_str_tokens(toks[1:])) for toks in current_tokens] # omit the BOS token\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_hooks(\n",
    "    model: HookedTransformer, \n",
    "    hooks: List[Tuple[str, Callable]],\n",
    "    prompts: List[Prompt],\n",
    "    n_tokens: int,\n",
    "    ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Apply the given hooks, and sample completions from the model.\n",
    "\n",
    "    This is used to check the \"hard\" accuracy of an intervention, i.e. whether\n",
    "    the intervention succesfully changes the model's output.\n",
    "    \"\"\"\n",
    "    texts = [p.text for p in prompts]\n",
    "\n",
    "    texts_tokens = model.to_tokens(texts, padding_side='left', prepend_bos=True)\n",
    "\n",
    "    # autoregressive generation w/ hook at each step\n",
    "    generated_tokens = []\n",
    "    current_tokens = texts_tokens\n",
    "    for i in range(n_tokens):\n",
    "        logits = model.run_with_hooks(current_tokens, fwd_hooks=hooks,)[:, -1, :] # only take the last token logits, shape (batch, vocab)\n",
    "        generated_tokens.append(logits.argmax(dim=-1))\n",
    "        current_tokens = torch.cat([current_tokens, generated_tokens[-1].unsqueeze(1)], dim=-1)\n",
    "    generated_tokens = torch.stack(generated_tokens, dim=1)\n",
    "    generated_texts = [''.join(model.to_str_tokens(toks, prepend_bos=False)) for toks in generated_tokens]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential binary mask on residual stream activations\n",
    "A prototype of training a binary mask baseline to change the value of an\n",
    "attribute by patching the residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 3 \n",
    "NUM_PROMPTS = 200\n",
    "# correct \n",
    "cf_labels = [correct_data.entity_attributes[p.entity][p.attribute] for p in cf_prompts]\n",
    "og_labels = [correct_data.entity_attributes[p.entity][p.attribute] for p in og_prompts]\n",
    "\n",
    "A_og = run_with_cache(\n",
    "    prompts_or_tokens=[p.text for p in og_prompts[:NUM_PROMPTS]],\n",
    "    model=model,\n",
    "    nodes=[ALL_NODES[LAYER]],\n",
    "    batch_size=None,\n",
    ")[0]\n",
    "\n",
    "A_cf = run_with_cache(\n",
    "    prompts_or_tokens=[p.text for p in cf_prompts[:NUM_PROMPTS]],\n",
    "    model=model,\n",
    "    nodes=[ALL_NODES[LAYER]],\n",
    "    batch_size=None,\n",
    ")[0]\n",
    "\n",
    "og_positions = Tensor([get_entity_positions(p, model)[-1] for p in tqdm(og_prompts[:NUM_PROMPTS])]).long()\n",
    "cf_positions = Tensor([get_entity_positions(p, model)[-1] for p in tqdm(cf_prompts[:NUM_PROMPTS])]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = DifferentialBinaryMasking( embed_dim=D_MODEL,).to(MODEL_DEVICE)\n",
    "optimizer = torch.optim.Adam(mask.parameters(), lr=1e-3)\n",
    "mask.train()\n",
    "EPOCHS = 100\n",
    "L1_PENALTY = 1e-3 # following RAVEL paper\n",
    "temperature_schedule = Tensor(np.linspace(1e-2, 1e-7, 100)) # following RAVEL paper\n",
    "for i in range(EPOCHS):\n",
    "    hook = get_dbm_hook(mask, LAYER, og_positions, A_og[:, og_positions, :], A_cf[:, cf_positions, :])\n",
    "    loss = get_loss_with_hooks(\n",
    "        model,\n",
    "        hooks=[hook],\n",
    "        prompts=og_prompts[:NUM_PROMPTS],\n",
    "        target_completions=cf_labels[:NUM_PROMPTS], \n",
    "        n_tokens=3,\n",
    "    )\n",
    "    with torch.no_grad(): # sample from the model to measure intervention success\n",
    "        completions = generate_with_hooks(\n",
    "            model,\n",
    "            hooks=[hook],\n",
    "            prompts=og_prompts[:NUM_PROMPTS],\n",
    "            n_tokens=10,\n",
    "        )\n",
    "        accs = [evaluate_completion(text=cf_prompts[i].text, completion=completions[i], expected_label=cf_labels[i]) for i in range(NUM_PROMPTS)]\n",
    "        acc = sum(accs) / len(accs)\n",
    "        print(f\"Accuracy: {acc:.2%}\")\n",
    "    loss = loss  # + L1_PENALTY * mask.get_sparsity_loss()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    mask.set_temperature(temperature_schedule[i])\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "mask.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing non-determinism bug\n",
    "Using Gemma-2B in bfloat16 resulted in non-deterministic behavior of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = DifferentialBinaryMasking(\n",
    "    embed_dim=2304,\n",
    ").to(MODEL_DEVICE)\n",
    "\n",
    "def get_mask_state(mask: DifferentialBinaryMasking) -> Dict[str, Tensor]:\n",
    "    return {k: v.detach().clone() for k, v in mask.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [p.text for p in og_prompts[:NUM_PROMPTS]]\n",
    "texts_tokens = model.to_tokens(texts, padding_side='left', prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.reset_hooks()\n",
    "    hook1 = get_dbm_hook(mask, LAYER, og_positions, A_og[:, og_positions, :], A_cf[:, cf_positions, :])\n",
    "    mask_state_1 = get_mask_state(mask)\n",
    "    logits1 = model.run_with_hooks(texts_tokens, fwd_hooks=[hook1],)[:, -1, :] # only take the last token logits, shape (batch, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.reset_hooks()\n",
    "    hook2 = get_dbm_hook(mask, LAYER, og_positions, A_og[:, og_positions, :], A_cf[:, cf_positions, :])\n",
    "    mask_state_2 = get_mask_state(mask)\n",
    "    logits2 = model.run_with_hooks(texts_tokens, fwd_hooks=[hook2],)[:, -1, :] # only take the last token logits, shape (batch, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask state should be the same\n",
    "for k in mask_state_1:\n",
    "    assert torch.all(mask_state_1[k] == mask_state_2[k])\n",
    "# but the logits are different\n",
    "logits1 - logits2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
